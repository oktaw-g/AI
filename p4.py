# -*- coding: utf-8 -*-
"""P4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SknG8AY-R29c6BudSZKH9VBZPTq3f0lE
"""

import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import numpy as np

DATASET_PATH = "BreastCancerData.csv"

"""#Wczytanie danych"""

# Wczytanie datasetu Breast Cancer
dataset = pd.read_csv(DATASET_PATH)

# Informacje o zestawie danych
dataset.info()

dataset = dataset.drop(columns="Unnamed: 32")

DATASET_FEATURE_LABELS = {
    'radius_mean': 'Średnia odległość środka od obwodu',
    'texture_mean': 'Odchylenie standardowe skali szarości',
    'perimeter_mean': 'Średni rozmiar rdzenia',
    'area_mean': 'Średni obszar',
    'smoothness_mean': 'Średnia gładkość',
    'compactness_mean': 'Średnia ścisłość(Kulowatość)',
    'concavity_mean': 'concavity_mean',
    'concave points_mean': 'concave points_mean',
    'symmetry_mean': 'Średnia symetria',
    'fractal_dimension_mean': 'fractal_dimension_mean',
}

# Zmiana nazwy kolumn
#dataset = dataset.rename(columns=DATASET_FEATURE_LABELS)
dataset.replace(to_replace=["M","B"], value = ['Złośliwy','Łagodny'], inplace=True)

#dataset['diagnosis'] = dataset['diagnosis'].astype('float')

dataset.info()

# Usunięcie zbędnych kolumn
dataset = dataset.drop(columns=['id','radius_worst','texture_worst','perimeter_worst','area_worst','smoothness_worst','compactness_worst','concavity_worst','concave points_worst','symmetry_worst','fractal_dimension_worst'])
dataset = dataset.drop(columns=['radius_se','texture_se','perimeter_se','area_se','smoothness_se','compactness_se','concavity_se','concave points_se','symmetry_se','fractal_dimension_se'])
dataset = dataset.drop(columns=['fractal_dimension_mean'])

#usunąć wszystkie worst
dataset.info()

"""#Brakujące dane"""

# Znalezienie brakujących danych
dataset.isnull().any()
# W rzędzie jest brakująca wartość
isnull = dataset.isnull().any(axis=1)
print(np.count_nonzero(isnull),"brakujących wartości.")
# Usunięcie rzędu
dataset = dataset.drop(np.asarray(isnull).nonzero()[0].tolist() ,axis=0)

# Podgląd pierwszych 10 rzędów
dataset.head(10)

"""#Wydzielenie zmiennej"""

# Wydzielenie zmiennej zależnej (Y)
x,y = dataset.drop(columns=["diagnosis"]), dataset["diagnosis"]

# Określenie liczby klas
counts = y.value_counts()
counts

# Współczynnik niezbalansowania
c_max, c_min = max(counts.values[0],counts.values[1]),min(counts.values[0],counts.values[1])
imbalance = c_max/c_min

print(f"Współczynnik niezbalansowania: {c_max} / {c_min} = {imbalance:.1f}")#zbiór jest delikatnie niezbilansowany 37% do 63%

"""EDA"""

# Podstawowa analiza statystyczna
x.describe()

# Histogramy zmiennych niezależnych
x.hist(figsize=(10,12), bins=20, edgecolor='black', grid=False)

# Analiza BOXPLOT

import seaborn as sns
import matplotlib.pyplot as plt

fig, axes = plt.subplots(1,3,figsize=(12,4))
sns.boxplot(x=y, y=x["radius_mean"], ax=axes[0])
sns.boxplot(x=y, y=x["perimeter_mean"], ax=axes[1])
sns.boxplot(x=y, y=x["texture_mean"], ax=axes[2])
plt.tight_layout()
plt.show()

fig, axes = plt.subplots(1,3,figsize=(12,4))
sns.boxplot(x=y, y=x["area_mean"], ax=axes[0])
sns.boxplot(x=y, y=x["smoothness_mean"], ax=axes[1])
sns.boxplot(x=y, y=x["compactness_mean"], ax=axes[2])
plt.tight_layout()
plt.show()

fig, axes = plt.subplots(1,3,figsize=(12,4))
sns.boxplot(x=y, y=x["concavity_mean"], ax=axes[0])
sns.boxplot(x=y, y=x["concave points_mean"], ax=axes[1])
sns.boxplot(x=y, y=x["symmetry_mean"], ax=axes[2])
#sns.boxplot(x=y, y=x["fractal_dimension_mean"], ax=axes[3])
plt.tight_layout()
plt.show()

# Analiza korelacji pomiędzy zmiennymi

import seaborn as sns
import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(10, 8))

sns.heatmap(x.select_dtypes(exclude='object').corr(), ax=ax, annot=True, fmt=".2f", cmap="coolwarm", center=0)

"""#Klasyfikacja"""

from sklearn.preprocessing import StandardScaler, OrdinalEncoder
from sklearn.compose import make_column_transformer


col_categorical = x.select_dtypes(include='object').columns
col_numerical = x.select_dtypes(exclude='object').columns

col_transformer = make_column_transformer(
    (StandardScaler(), col_numerical),
    (OrdinalEncoder(), col_categorical)
)

from  sklearn.preprocessing import LabelEncoder

# Enkodowanie zmiennej zależnej
le = LabelEncoder()
y = le.fit_transform(y)
y_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))
y_name_mapping

from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline

svc = Pipeline([
    ('col_transformer', col_transformer),
    ('svc', SVC(kernel="linear", C=1.0)),
])

knn = Pipeline([
    ('col_transformer', col_transformer),
    ('knn', KNeighborsClassifier(n_neighbors=3)),
])

"""#Walidacja krzyżowa"""

from sklearn.model_selection import cross_validate

SCORING = ('accuracy', 'recall', 'precision', 'f1')

results = cross_validate(svc, x, y, cv=5, scoring=SCORING)

print("=====\nSVC:\n=====")
for key in results.keys():
    print(f"{key}: {results[key].mean():.3f}")

results = cross_validate(knn, x, y, cv=5, scoring=SCORING)

print("=====\nk-NN:\n=====")
for key in results.keys():
    print(f"{key}: {results[key].mean():.3f}")

"""#Gridsearch"""

# Listowanie parametrów SVC
svc['svc'].get_params()

# Listowanie parametrów k-NN
knn['knn'].get_params()

# Tuning hiperparametrów k-NN z wykorzystaniem GridSearchCV
from sklearn.model_selection import GridSearchCV

parameters = {
    'knn__n_neighbors': [1, 3, 5, 7, 9, 11],
    'knn__metric': ['euclidean', 'cosine', 'cityblock', 'l1', 'l2', 'nan_euclidean'],
}
grid_search = GridSearchCV(knn, parameters)
grid_search.fit(x, y)

# Zastosowanie hiperparametrów
for param, val in grid_search.best_params_.items():
    knn.set_params(**{param: val})

grid_search.best_params_

# Tuning hiperparametrów SVC z wykorzystaniem GridSearchCV
from sklearn.model_selection import GridSearchCV

parameters = {
    'svc__C': list(np.arange(0.1, 2.1, 0.1)),
    'svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
}
grid_search = GridSearchCV(svc, parameters)
grid_search.fit(x, y)

# Zastosowanie hiperparametrów
for param, val in grid_search.best_params_.items():
    svc.set_params(**{param: val})

grid_search.best_params_

results = cross_validate(svc, x, y, cv=5, scoring=SCORING)

print("=====\nSVC:\n=====")
for key in results.keys():
    print(f"{key}: {results[key].mean():.3f}")

results = cross_validate(knn, x, y, cv=5, scoring=SCORING)

print("=====\nk-NN:\n=====")
for key in results.keys():
    print(f"{key}: {results[key].mean():.3f}")